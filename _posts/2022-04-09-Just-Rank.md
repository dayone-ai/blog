---
date: 2022-03-24 12:26:40
layout: post
title: "Just Rank: Rethinking Evaluation with Word and Sentence Similarities"
subtitle:
description: hello
image: /assets/img/posts/Just-Rank/just_rank_img.png
optimized_image: /assets/img/posts/Just-Rank/just_rank_img.png
category: Deep Learning
tags:
  - Deep Learning Paper
  - nlp-sentence-embedding
author: dayone
paginate: true
comment: true
---


#{% katexmm %}

- [Paper Link](https://arxiv.org/pdf/2203.02679v2.pdf)
- [Takeaways](#takeaways)
- [Approach](#approach)
    - [💡 Theory and Motivation](#-theory-and-motivation)
    - [EvalRank](#evalrank)
    - [Methodology](#methodology)
    - [Datasets Collection](#datasets-collection)
    - [Good Intrinsic Evaluator](#good-intrinsic-evaluator)
- [Result](#result)
    - [Sentence Level Experiments](#sentence-level-experiments)
    - [Sentence Level Result](#sentence-level-result)
- [Background Knowledge](#background-knowledge)
    - [기존 embedding 구하는 방법](#기존-embedding-구하는-방법)
    - [문제점](#문제점)


# Takeaways

---

- retrieval등의 downstream task에 쓰이는 embedding을 뽑기 위한 encoder학습에 sts의 대안인 `EvalRank`를 제시함. 실험해볼 가치가 있다고 생각.
- spreading-activation theory에서 영감을 받음
- extensive experiments with 60+ models and 10 downstream tasks

# Approach

---

### 💡 Theory and Motivation
 - Spreading-Activation Theory: explains how concepts store and interact within the human brain
 the concept network
   - only highly related concepts are connected
   -To find the relatedness between concepts like engine and street, the activation is spreading through mediating concepts like car and ambulance with decaying factors.
 - To test the soundness of the concept network, it is enough to ensure the local connectivity between concepts
 - the long-distance relationships can be inferred with various spreading activation algorithms


![Untitled](/assets/img/posts/Just-Rank/Untitled.png)

### EvalRank

- test only on highly related pairs and make sure they are topologically close in the embedding spac
- 기술된 문제점 완화
    1. focus on highly related pairs, which are intuitive to human annotators
    2. stronger correlation with downstream tasks as desired properties are measured
    3. less affected by the whitening methods (treat the embedding space from a local perspective)

### Methodology

- frame the evaluation of embeddings as a retrieval task
- EvalRank dataset
    1. {% katex %} P = \{p_1, p_2 . . . p_m\} {% endkatex %}
    2. {% katex %}C =  \{c_1, c_2, . . . c_n\}{% endkatex %}
    - Each positive pair {% katex %}p_i{% endkatex %}  in {% katex %}P{% endkatex %} consists of two samples in {% katex %}C{% endkatex %} that are semantically similar:
    {% katex %}p_i = (c_x, c_y){% endkatex %}
- For each sample $c_x$ and its positive correspondence {% katex %}c_y{% endkatex %} a good embedding model should has their embeddings {% katex %}(e_x, e_y){% endkatex %} close in the embedding space.
    - the other background smaples should locate farther away from the sameple $c_x$
- the similarity score is used to sort all background samples in descending order
    - the performance at each positive pair {% katex %}p_i{% endkatex %} is measured by the rank of $c_x$ ‘s positive correspondence $c_y$ w.r.t all background samples
    - $rank_i = rank(S(c_x, c_y), [||_{j=1, j \ne x}^n s(c_x, c_j)])$
        - $||$ : concatenate operation
    - To measure the overall performance MRR and Hits@k scores are used
        - $MRR = \frac{1}{m} \Sigma_{i=1}^m \frac{1}{rank_i}$
        - $Hits@k = \frac{1}{m} \Sigma_{i=1}^k 1[rank_i \leq k]$

### Datasets Collection

- STS-Benchmark, STR dataset에서 상위 25% 유사도를 보이는 pair들이 positive pair로 쓰임
- reversed pair도 positive pair로 쓰임

### Good Intrinsic Evaluator

- EvalRank가 더 좋은 embedding의 indicator임을 보여주기 위해, word/sentence embedding의 성능을 측정하는 intrinsic evaluator (sts, EvalRank)와 downstream task에서의 성능을 측정한 후 그 상관관계를 측정함
- EvalRank가 더 좋은 지표이기 위해서는 EvalRank와 downstream tasks의 성능이 높은 상관관계를 보여야 한다.

# Result

---

### Sentence Level Experiments

- Setup
    - 67 embedding models

### Sentence Level Result

- Downstream task performance와의 상관관계

    ![Untitled](/assets/img/posts/Just-Rank/Untitled%201.png)

    - EvalRank outperforms all sts datasets with a clear margin
- Visualization

    ![Untitled](/assets/img/posts/Just-Rank/Untitled%202.png)

    - Model performance on MR is pivot
        - MR task에서 잘하는 순서대로 ranking을 했음
        - SST 는 유사한 sentiment analysis라서 높은 상관관계를 보임
        - STS-B나 STR에 비해서 파란색 EvalRank가 더 높은 상관관계를 보인다
        -

# Background Knowledge

---

### 기존 embedding 구하는 방법

1. word/sentence embedding을 구한다 (주로 language model)
2. word/sentence pair의 similarity를 구한다
    - Cosine similarity가 대부분의 case에서 쓰임
3. similarity score를 human annotation의 gt와 correlation을 구한다

### 문제점

- sentence similarity기반의 embedding을 구하는 방법은 downstream task에 부정적인 영향을 미칠 수 있음
    1. the definition of similarity is too vague (multifaceted relationships)
        1. the concept of similarity and relatedness are not well-defined
            1. 유사한 문장은 연관되어있지만 연관되었다고 유사하지는 않다.
            2. relationships between samples are far more complicated than currently considered, which is challenge to all current datasets
        2. the annotation process is not intuitive to humans
            1. the instructions on similarity levels are not well defined
            2. priming effect theory: 사람은 비교를 잘하기 위해서는 pivot sample이 필요하다.
                1. it is more intutitive for human to compare $(a, b) > (a,c)$ than $(a,b) > (c, d)$ as far as similarity is concerned
                2. 코끼리와 팽귄의 유사도 vs 아메리카노와 그린티 스무디의 유사도
    2. the similarity evaluation tasks are not directly related to downstream tasks
        1. low testing corpus overlap
            1. sts corpus는 그들만의 source가 있다
        2. mismatch of tested properties
            1. sts의 학습목표가 downstream task에서는 관심사가 아닐 수 있다.
                1. sts는 가장 효과적인 sts system을 찾기 위한 것이었지, sentence embedding을 효과적으로 찾기 위한 것이 아니었다.
    3. the evaluation paradigm can be tricked with simple post-processing methods (overfitting)
        1. similarity metric을 `cosine similarity` → `l2`로 바꾸는 것 만으로도 모델들의 순위가 바뀐다.

            ![Untitled](/assets/img/posts/Just-Rank/Untitled%203.png)

        2. whitening tricks
            1. 흔히 쓰는 기법: to obtain a more isotropic embedding space and can be summarized as a space **whitening process**
            2. whitening이 sts에는 도움이 되지만 downstream task에는 도움이 되지 않는다
            3. whitening 기법이 유사도가 낮은 sample들의 성능을 올리는 데만 도움이 되는 듯 하다.

                ![Untitled](/assets/img/posts/Just-Rank/Untitled%204.png)

#{% endkatexmm %}